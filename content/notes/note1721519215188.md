# Intent Classification

# Intent Classification Overview

Modern intent classification methods capitalize on large datasets and compute power. However, I have essentially no data- all data is synthetically generated- and one GPU. So I have to be a little creative when implementing intent classification. I started out using Bert-Uncased and trying to train a standard model, but as everyone in ML knows standard models accuracy is directly proportional to data size and quality. Given that I need the model to be fast and accurate on sub-par data, I went for a less complicated method.

## Grouping Intents to Increase Accuracy

Need RAG algorithms for journeys.

**Intents:**
1. General question (none)
2. Goodbye (none)
3. Journey exploration (j-set, user-info)
4. Feedback and reflection (user-info)
5. Goal setting (none)
6. Question (j-set, user-info)
7. Greeting (none)
8. Clarification (immediate chat history)
9. Resource recommendation (j-set, user-info)
10. App (app structure and info)
11. Progress tracking (j-set, user-info)
12. Answering (user-info)

**Datasets:**
1. J-set
2. User info
3. Immediate chat history
4. App structure and info

**Groups:**
- (none)
  - 2. Goodbye
  - 7. Greeting
- (j-set, user-info)
  - 3. Journey exploration
  - 6. Question
  - 9. Resource recommendation
  - 11. Progress tracking
  - 5. Goal setting
  - 4. Feedback and reflection
  - 12. Answering
- (immediate chat history)
  - 1. General question
  - 8. Clarification
- (app structure and info)
  - 10. App

## Arbitrary Todo-List

- [x] Change the base prompts: there are instances where the context is not relevant to the query.
- [x] Tell the model that the context sometimes will not be applicable to answer the query.
- [x] Focus on query answering. Responses are too un-engaging.
- [x] Also explicitly state that the journey is not about that user.

## Technical Implementation

### High-Level Overview

The intent classification and response generation system combines sentence embeddings and cosine similarity to classify intents and improve the accuracy of responses. This system is designed to work efficiently with a small dataset by leveraging embedding-based similarity rather than a traditional large-scale BERT model. 

### Intent Classification Process

1. **Data Preprocessing:** Load and preprocess intent data and messages.
2. **Embedding Generation:** Use `SentenceTransformer` to generate embeddings for messages and contexts.
3. **Cosine Similarity:** Calculate cosine similarity between query embeddings and precomputed intent embeddings to classify intents.
4. **Dynamic Prompt Selection:** Select and format the appropriate prompt based on the classified intent and relevant context.

### Detailed Code Explanation

#### Data Preprocessing and Embedding Generation

``` python
def preprocess_data(self):
    self.titles = {index: journey['Journey'] for index, journey in enumerate(self.data['journeys'])}
    self.titles_text = list(self.titles.values())
    self.titles_embeddings = self.embedding_model.encode(self.titles_text, convert_to_tensor=True).to("cuda")

    self.dict_list = {}
    self.dict_list_embeddings = {}

    for item in self.intent_data:
        intent = item['intent']
        context = item.get('context', " ")
        message = item['message']
        context_message_pair = (context, message)
        if context == None:
            context = " "
        context_message_pair_embed = (
            self.embedding_model.encode(context, convert_to_tensor=True).to('cuda'),
            self.embedding_model.encode(message, convert_to_tensor=True).to('cuda')
        )

        self.dict_list.setdefault(intent, []).append(context_message_pair)
        self.dict_list_embeddings.setdefault(intent, []).append(context_message_pair_embed)

    self.average_embeddings = self.get_average_intent_embedding()
```

#### Calculating Average Embeddings for Intents

``` python
def get_average_intent_embedding(self):
    return {
        intent: torch.mean(torch.stack([embedding for pair in embeddings for embedding in pair]), dim=0)
        for intent, embeddings in self.dict_list_embeddings.items()
    }
```

#### Classifying Intents Using Cosine Similarity

``` python
def classify_intent(self, query):
    query_embedding = self.embedding_model.encode(query, convert_to_tensor=True).to('cuda')
    best_intent = max(self.average_embeddings.items(), key=lambda x: util.cos_sim(a=query_embedding, b=x[1])[0])[0]
    self.intent = best_intent
    return best_intent
```

#### Retrieving Relevant Content Based on Intent

``` python preview title="chat template"
def get_content(self, query):
    group = self.get_group()
    if group in ("none", "immediate_chat_history"):
        return None
    elif group == "app":
        app_info_pdf = fitz.open('data/app_info.pdf')
        app_info = ""
        for page_num in range(len(app_info_pdf)):
            page = app_info_pdf.load_page(page_num)
            app_info += page.get_text()
        return app_info
    top_titles, top_results, query_embedding = self.get_relevant_journeys(query)
    top_steps = self.get_relevant_steps(query, top_titles, top_results, query_embedding)

    top_steps_embeddings = self.embedding_model.encode(top_steps, convert_to_tensor=True).to("cuda")
    cos_score_steps = util.cos_sim(a=query_embedding, b=top_steps_embeddings)[0]
    top_results_steps = torch.topk(cos_score_steps, k=5)
    top_idx = top_results_steps[1][0]

    self.top_titles = top_titles[top_idx]

    return top_steps[top_idx]
```

### Challenges and Solutions

**Problem:** Handling intents with small datasets and ensuring accurate classification.

**Solution:** By utilizing cosine similarity on sentence embeddings, the system can accurately classify intents without requiring large datasets. Additionally, the use of dynamic prompts based on classified intents ensures relevant and contextually appropriate responses.

This approach effectively balances the constraints of limited data and computational resources while maintaining high accuracy and responsiveness in intent classification and response generation.