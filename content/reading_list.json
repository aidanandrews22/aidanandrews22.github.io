[
  {
    "id": "transformers-in-context-learning",
    "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes",
    "url": "https://arxiv.org/pdf/2208.01066",
    "tags": ["Berkeley Research"],
    "dateAdded": "2025-05-30"
  },
  {
    "id": "in-context-imitation-learning",
    "title": "In-Context Imitation Learning via Next-Token Prediction",
    "url": "https://arxiv.org/pdf/2408.15980",
    "tags": ["Berkeley Research"],
    "dateAdded": "2025-06-09"
  },
  {
    "id": "transformer-mpc",
    "title": "Transformer-based Model Predictive Control: Trajectory Optimization via Sequence Modeling",
    "url": "https://arxiv.org/pdf/2410.23916v1",
    "tags": ["Berkeley Research"],
    "dateAdded": "2025-06-12"
  },
  {
    "id": "prompt-robot-walk-llm",
    "title": "Prompt a Robot to Walk with Large Language Models",
    "url": "https://arxiv.org/pdf/2309.09969",
    "tags": ["Berkeley Research"],
    "dateAdded": "2025-06-14"
  },
  {
    "id": "attention-is-all-you-need",
    "title": "Attention Is All You Need",
    "url": "https://arxiv.org/pdf/1706.03762",
    "tags": ["Personal"],
    "dateAdded": "2024-02-10"
  },
  {
    "id": "imagenet-classification-with-deep-convolutional-neural-networks",
    "title": "ImageNet Classification with Deep Convolutional Neural Networks",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf",
    "tags": ["Personal"],
    "dateAdded": "2024-01-30"
  },
  {
    "id": "openvla-vision-language-action",
    "title": "OpenVLA: An Open-Source Vision-Language-Action Model",
    "url": "https://arxiv.org/abs/2406.09246",
    "tags": ["Robotics", "Vision-Language-Action"],
    "dateAdded": "2025-08-15"
  },
  {
    "id": "navila-legged-robot-navigation",
    "title": "NaVILA: Legged Robot Vision-Language-Action Model for Navigation",
    "url": "https://navila-bot.github.io/static/navila_paper.pdf",
    "tags": ["Robotics", "Vision-Language-Action", "Navigation"],
    "dateAdded": "2025-09-03"
  },
  {
    "id": "hierarchical-vision-language-planning",
    "title": "Hierarchical Vision-Language Planning for Multi-Step Humanoid Manipulation",
    "url": "https://arxiv.org/html/2506.22827v1",
    "tags": ["Robotics", "Vision-Language-Action", "Manipulation"],
    "dateAdded": "2025-08-28"
  },
  {
    "id": "groot-n1-humanoid-robots",
    "title": "GR00T N1: An Open Foundation Model for Generalist Humanoid Robots",
    "url": "https://arxiv.org/abs/2503.14734",
    "tags": ["Robotics", "Humanoid", "Foundation Model"],
    "dateAdded": "2025-09-18"
  },
  {
    "id": "groot-n1-5-humanoid-robots",
    "title": "GR00T N1.5",
    "url": "https://research.nvidia.com/labs/gear/gr00t-n1_5/",
    "tags": ["Robotics", "Humanoid", "Foundation Model"],
    "dateAdded": "2025-09-01"
  },
  {
    "id": "lever-humanoid-whole-body-control",
    "title": "LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction",
    "url": "https://arxiv.org/abs/2506.13751",
    "tags": ["Robotics", "Humanoid", "Whole-Body Control"],
    "dateAdded": "2025-08-22"
  },
  {
    "id": "pi05-vision-language-action",
    "title": "Ï€0.5: a Vision-Language-Action Model with Open-World Generalization",
    "url": "https://www.physicalintelligence.company/download/pi05.pdf",
    "tags": ["Robotics", "Vision-Language-Action", "Generalization"],
    "dateAdded": "2025-09-12"
  }
]
